**This is my reproduction of the results from https://github.com/hustvl/EVF-SAM**

**Abstract:**

This paper presents EVF-SAM, a state-of-the-art model designed for referring expression segmentation and  by leveraging multimodal prompts that combine text and image information. EVF-SAM integrates powerful vision-language models like BEIT-3 with the Segment Anything Model (SAM) to achieve superior segmentation accuracy. Despite its innovative architecture, it shows robust generalization across challenging RES benchmarks and this sets new standards for multimodal segmentation tasks. This paper  highlights the potential of combining foundational vision and language models for real-world applications.


---

# Results


![image](https://github.com/user-attachments/assets/208e148a-d21a-4f3f-a657-c387556009d7)


